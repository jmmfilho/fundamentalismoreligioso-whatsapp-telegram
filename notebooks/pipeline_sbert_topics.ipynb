{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386d0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nasci\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhdbscan\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregar dataset\n",
    "df = pd.read_csv('../datasets/fakeWhatsApp.BR_2022.csv', delimiter=',', index_col=0)\n",
    "\n",
    "# IDs a remover\n",
    "ids_para_remover = [\n",
    "    '6ef561ec0f448afcd7b3751124bb0712', 'd3e678a0ba0e1485548260a7c4599152',\n",
    "    'ac5703154484de05336af617455ca55e', 'a39edbd64d378226ffa60433649a0acf',\n",
    "    'c66d0d4ae5a4b281bff67e1fa4fbd6ba', '819bbc872ed6d81f44d746b710eecf06',\n",
    "    'ec94da4d54f9a5693e88fa582926be53', '6a38c72316d87c028dfd66c10442476b',\n",
    "    '94099e1e46f129856541e2b3640896d1', '8f367d1693fff47218603fa47ded525c',\n",
    "    'c09caffee0d1bd30926dea9df25dc88f'\n",
    "]\n",
    "\n",
    "df_filtrado = df[~df['id_member_anonymous'].isin(ids_para_remover)]\n",
    "df_filtrado = df_filtrado.dropna(subset=['pre_processed_text'])\n",
    "df_filtrado = df_filtrado[df_filtrado['trava_zap'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    def substituir_dominios(texto):\n",
    "        def extrair_dominio(url):\n",
    "            dominio = re.sub(r'^https?://(?:www\\.)?|www\\.', '', url)\n",
    "            dominio = re.split(r'[/?#]', dominio)[0]\n",
    "            return dominio.split('.')[0]\n",
    "        return re.sub(r'https?://(?:www\\.)?\\S+|www\\.\\S+', lambda match: extrair_dominio(match.group(0)), texto)\n",
    "\n",
    "    text = substituir_dominios(text)\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'([\\U00010000-\\U0010FFFF])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'k{2,}|K{2,}', 'kk', text)\n",
    "    text = re.sub(r'(ha){2,}', 'haha', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(ka){2,}', 'kaka', text, flags=re.IGNORECASE)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_geral = df_filtrado.copy()\n",
    "df_geral['text_processed'] = df_geral['pre_processed_text'].apply(preprocess_text)\n",
    "docs = df_geral['text_processed'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8da18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = model.encode(docs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af908b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=42)\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "cluster_model = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean', prediction_data=True)\n",
    "labels = cluster_model.fit_predict(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clustered = pd.DataFrame({'text': docs, 'label': labels})\n",
    "topics_words = []\n",
    "\n",
    "for label in sorted(set(labels)):\n",
    "    if label == -1: continue\n",
    "    texts = df_clustered[df_clustered['label'] == label]['text']\n",
    "    all_words = \" \".join(texts).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    top_words = [word for word, _ in word_freq.most_common(10)]\n",
    "    topics_words.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c57ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [doc.split() for doc in docs]\n",
    "id2word = Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "coh_cv = CoherenceModel(topics=topics_words, texts=texts, dictionary=id2word, coherence='c_v').get_coherence()\n",
    "coh_npmi = CoherenceModel(topics=topics_words, texts=texts, dictionary=id2word, coherence='c_npmi').get_coherence()\n",
    "coh_uci = CoherenceModel(topics=topics_words, texts=texts, dictionary=id2word, coherence='c_uci').get_coherence()\n",
    "coh_umass = CoherenceModel(topics=topics_words, corpus=corpus, dictionary=id2word, coherence='u_mass').get_coherence()\n",
    "\n",
    "def topic_diversity(topics_words, top_n=10):\n",
    "    words = [word for topic in topics_words for word in topic[:top_n]]\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / (len(topics_words) * top_n)\n",
    "\n",
    "div = topic_diversity(topics_words)\n",
    "\n",
    "def irbo(topics, topk=10, p=0.9):\n",
    "    def rbo(list1, list2, p):\n",
    "        overlap = 0.0\n",
    "        rbo_score = 0.0\n",
    "        depth = min(len(list1), len(list2))\n",
    "        for d in range(1, depth + 1):\n",
    "            if list1[d-1] in list2[:d] and list2[d-1] in list1[:d]:\n",
    "                overlap += 1\n",
    "            rbo_score += overlap / d * (p ** d)\n",
    "        return (1 - p) * rbo_score\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            t1 = topics[i][:topk]\n",
    "            t2 = topics[j][:topk]\n",
    "            score = rbo(t1, t2, p)\n",
    "            scores.append(score)\n",
    "    return 1 - np.mean(scores) if scores else 0.0\n",
    "\n",
    "irbo_score = irbo(topics_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nðŸ“Š AvaliaÃ§Ã£o dos TÃ³picos:\")\n",
    "print(f\"CoerÃªncia c_v:     {coh_cv:.4f}\")\n",
    "print(f\"CoerÃªncia c_npmi:  {coh_npmi:.4f}\")\n",
    "print(f\"CoerÃªncia c_uci:   {coh_uci:.4f}\")\n",
    "print(f\"CoerÃªncia u_mass:  {coh_umass:.4f}\")\n",
    "print(f\"Diversidade:       {div:.4f}\")\n",
    "print(f\"IRBO:              {irbo_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
